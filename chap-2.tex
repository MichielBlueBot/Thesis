\chapter{Integration Strategies}
\label{cha:integration}

\section{Introduction}
\label{sec:integration-introduction}
Current advancements in technology are leading to increasingly larger datasets. This is a trend that is becoming very apparent in many different fields of research. One of these is the biomedical field. Technology is about to reach the point where we are able to sequence anyones DNA at a very low cost. Secondly, we have a range of highly advanced imaging instruments creating massive amounts of imaging features. And on top of that, there are big discussions going on about researchers having to make their datasets open to the public to help other researchers around the world. All of these trends lead to huge amounts of data from different sources becoming available for analysis. This raises the question of how we have to integrate data from these different sources to get the most information out of them. In this chapter I will first explain the general setup that we are dealing with, followed by three different integration strategies that show how we can combine datasets from different sources.

\section{The setup}
We have a set of D datasets, each dataset can be represented by a matrix where the rows are the samples and the columns are the explanatory variables. Each dataset can have a different number of explanatory variables, but they all have the same amount of rows such that samples can be linked across datasets. For instance: imagine we have a set of 200 cancer patients. Each of these patients has had images taken of the cancer by different machines: an MRI scan, a PET scan, ... and they also had a bloodtest done measuring different prote\"{i}ne levels in the blood (an ELISA test). Each of these methods will yield a dataset: one for each imaging scan and one for the blood test. The explanatory variables are obviously different for each dataset: the imaging datasets will have image features like blobs and pixel intensities, while the bloodtest will have values that represent the prote\"{i}ne levels in the blood. Each dataset will however have 200 rows, one for each patient. \\ \\
Now that we have our input data, we need a target function (dependent variable). In the example case this could be whether the patient fully recovers from the treatment or not. This variable has a binomial distribution and thus we could use logistic regression to try and estimate this variable. The task of integration is now the following: how do we combine the samples across the different datasets to come up with a model that uses the information in all the datasets? In the following sections I will present three ways to do this: the early integration method which is the current standard, the late integration method that is based on an ensemble of models, and the intermediate integration method that takes advantage of variable selection present in the lasso regularization.

\section{Early integration}
\label{sec:integration-early}
The first integration method is called early integration and it is the method that is most widely uses at the time. It is also the easiest way of dealing with different data sources as we are simply going to concatenate the data for each sample. Figure \ref{fig:integration-early} shows the schema for early integration of data sources. A new large dataset is constructed by concatenating the individual datasets by matching sample. The number of samples obviously stays the same, the number of explanatory variables is equal to the sum of explanatory variables of all individual datasets. This integrated dataset can then be used to train the final model directly.
\begin{figure}
	\centering
	\includegraphics[scale=1]{images/early_integration}
	\caption{Scheme for early integration}
	\label{fig:integration-early}
\end{figure}
\section{Late integration}
\label{sec:integration-late}
In late integration, as the name suggests, we will combine the different datasets at the end of the learning process. The schema for late integration is shown on figure \ref{fig:integration-late}. First, a model is learned for each dataset individually. This gives rise to $D$ different models. When we want to make a prediction using this set of models, we will ask each model to provide an output individually and then combine them using a linear combination. If we have no preference for any model we could simply compute the average output. This average would then be our final output for the integrated model. In this case we cannot represent the final model by some model parameters (weights) but rather we have to view the full set of $D$ models as well as the linear combination we chose as the full integrated model. \\ \\
There is an analogy between late integration and the ensemble averaging technique in machine learning. Ensemble learning means that instead of just building one model for a dataset, we build several models for the same dataset and then average their outcomes. Late integration takes this idea but applies it to a set of data sources instead of just one. \\ \\
The thought behind this model is that we try to learn as much as possible from each dataset individually, before we combine them. An example of this is that the variable selection by the lasso regularization now has the opportunity to select the best variables for each dataset individually. While in the case of early integration, all the variables from the datasets are competing against each other at the same time to make it into the final model.
\begin{figure}
	\centering
	\includegraphics[scale=1]{images/late_integration}
	\caption{Scheme for late integration}
	\label{fig:integration-late}
\end{figure}
\section{Intermediate integration}
\label{sec:integration-intermediate}
Intermediate integration is possibly the most advanced integration method presented. It tries to take advantage of the variable selection when using a lasso penalty. The schema for intermediate integration is shown on figure \ref{fig:integration-intermediate}. It starts out the same way as late integration, for each dataset we compute an individual model. However, instead of using the output of these models we will simply look at the variables that were chosen by each model to be informative. We will then go back to the original datasets and extract only those explanatory variables and concatenate them together into a new integrated dataset. Lastly, we use this integrated dataset to learn a final model. \\ \\
The intermediate integration can be seen as a two-step learning process. First we preprocess the datasets by computing individual models and we extract only the informative variables. Then we will train our final model on the reduced dataset. This is very advantageous if the initial datasets have a huge amount of explanatory variables, as the preprocessing step will reduce this amount drastically while still keeping as much useful information as possible.
\begin{figure}
	\centering
	\includegraphics[scale=1]{images/intermediate_integration}
	\caption{Scheme for intermediate integration}
	\label{fig:integration-intermediate}
\end{figure}
\section{Conclusion}
\label{sec:integration-conclusion}
In this chapter I have shown the issue of large and high-dimensional datasets that we are currently facing. I have presented three different strategies to deal with this issue. The early integration method is the na\"{i}ve method that simply concatenates the datasets together. The late integration method uses an analogous technique to ensemble learning in order to extract as much information as possible. Lastly, the intermediate integration technique introduces a two-step learning method that takes advantage of variable selection to reduce the dimensionality of the datasets.
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
