\chapter{Evaluation of integration strategies}
\label{cha:evaluation}


\section{Introduction}
\label{sec:evaluation-introduction}
In this chapter we will show that different integration strategies can have better performance in different situations. We will do this by computing a model using each strategy and then comparing their performance using an appropriate metric.
\section{Evaluating a logistic regression model}
\label{sec:evaluation-logisticregression}
The first evaluation will be made using a logistic regression model. This is a model computed for a dependent variable that has a binomial distribution. To evaluate such model, a common technique used is called Receiver-Operating-Characteristic analysis\cite{zweig1993receiver}\cite{hajian2013receiver}\cite{wikiroc} or ROC analysis for short. Remember that when we are using a logistic regression model to predict outcomes, we have to define a threshold to separate positive from negative cases. An ROC curve shows the performance of the model for any possible threshold, using metrics such as sensitivity and specificity. This gives a very good general impression of the performance of the model.
\subsection{Predictions using validation}
What we really want to do is estimate the out-of-sample error for our model. To do this we have seen that we can use the technique of cross-validation. In this case we are not going to compute any learning parameter but we are going to use the validation set as a test set. We will split our dataset into $K$ folds. We will train a model on $K-1$ folds and we will use this model to predict the outcomes of the remaining samples in the test fold. By repeating this process for each folds we obtain a predicted outcome for each sample in the dataset. Notice however that we also have the real outcome of the samples in our dataset, and thus we can compare our predictions with the real values to estimate our out-of-sample error.
\subsection{Metrics}
When we compare our models predictions with the real outcomes, there are four possible scenario's:
\begin{itemize}
	\item True Positive (TP): the model predicted positive and this is correct
	\item True Negative (TN): the model predicted negative and this is correct
	\item False Positive(FP): the model predicted positive and this is wrong (type 1 error)
	\item False Negative (FN): the model predicted negative and this is wrong (type 2 error)
\end{itemize}
Notice that this gives us more information than just registering whether we are right or wrong. It also tells us the type of mistake we made. This is a very important distinction because depending on the application there is usually a different cost attached to making type 1 or type 2 errors. And as we will see later on, there is a tradeoff between the two and we can tune our system to be more resistant towards making one type of error. \\ \\
Now that we have established the notion of typed errors, there are several ways in which these numbers are combined to form evaluation metrics. Depending on your field of research these often get different names, we will use the ones applicable to the biomedical field.
\subsubsection{Sensitivity}
Sensitivity\cite{wikisensspec} measures the proportion of the real positive samples that our model correctly predicted as positive. It can be calculated with the following formula:
$$
Sensitivity = \frac{\sum{TP}}{\sum{P}}
$$
where
\begin{itemize}
	\item $\sum{TP}$ is the number of true positives
	\item $\sum{P}$ is the number of real positive cases in the dataset
\end{itemize}
Sensitivity can be thought of, as its name suggests, as how sensitive the model is to detecting positive cases. If the model gets a real positive sample's input, what is the probability that it will detect it as such. A sensitivity of 1 means that our model is capable of correctly identifying all positive cases in the dataset. This means that the higher the sensitivity, the lower the type 2 error rate.
\subsubsection{Specificity}
Specificity\cite{wikisensspec} is the analogous metric to sensitivity, but for negative cases. It measures the proportion of the real negative samples that our model correctly predicts as negative.
$$
Specificity = \frac{\sum{TN}}{\sum{N}}
$$
where
\begin{itemize}
\item $\sum{TN}$ is the number of true negatives
\item $\sum{P}$ is the number of real negative cases in the dataset
\end{itemize}
A specificity of 1 means that our model is capable of correctly identifying all negative cases in the dataset. This means that the higher the specificity, the lower the type 1 error rate.
\subsubsection{The tradeoff}
At this point it is obvious that we would like to maximize both sensitivity and specificity. Indeed, if both metrics are 1 then we make no errors and we have a perfect model. In practice however this is nearly impossible to achieve. It is easy to see that there is a tradeoff between the two metrics: when we try to increase the sensitivity, we will lose out on specificity and vice versa. \\ \\ 
Consider the totally useless model that has a threshold lower than the smallest possible output. Meaning that this model will always predict a positive outcome regardless of the input. This model will have a sensitivity of 1, as we will get all positive cases correct. But it will also have a specificity of 0 because we get none of the negative cases correct. As we gradually increase the threshold we will cross critical values where some sample inputs will now produce an output below the threshold and thus be classified as negatives. If this happens to be a correct prediction our specificity will go up and sensitivity will remain unchanged. If it happens to be an error, specificity is unchanged and sensitivity will go down. The gradual increase of the threshold will thus cause sensitivity to drop and specificity to increase, up to the point where we reach the other extreme. The threshold is now bigger than the biggest possible output and the model always predicts negative regardless of the input. This threshold gives us a sensitivity of 0 and a specificity of 1.
\subsection{Receiver Operating Characteristic curve}
The ROC curve is a way of showing exactly this gradual increase of the threshold. The typical way of constructing a ROC curve is to put $(1-specificity)$ on the X-axis and $sensitivity$ on the Y axis. The values for these matrics are then plotted for values of the threshold ranging from the all-positive model to the all-negative model. An example ROC curve is shown on figure \ref{fig:evaluation-roc}. The metric we will use to evaluate the performance of different models is the area under the ROC curve (AUC). This metric works because it indicates how far we can optimize both sensitivity and specificity for different thresholds. It shows how close we can get to the upper left corner of the plot, which indicates the perfect model. The AUC metric ranges from 0 to 1, but in fact its value is only informative in between 0.5 and 1. Imagine a model that makes completely random predictions, this model will get a correct prediction 50\% of the time. If we would create a ROC curve for this model we would on average get a straight diagonal line from the bottom left to the top right. This line is therefore considered a lower bound for the performance of a model.
\begin{figure}
	\centering
	\includegraphics[scale=.9]{images/roc_curve}
	\caption{Example ROC curve and metrics}
	\label{fig:evaluation-roc}
\end{figure}
\section{Case study 1: predicting stage outcome}
\label{sec:evaluation-predictingstage}
\subsection{Case details}
\label{sec:evaluation-casedetails}
The case we are showing here uses datasets created by the University Hospital of Leuven in a study on colorectal cancer patients. There are three source datasets that all contain imaging data: a dataset from MRI (Magnetic Resonance Imaging) scans, a dataset from DWI (Diffusion Weighted Magnetic Resonance Imaging) scans, and a dataset from PET (Positron Emission Tomography) scans. It is important to note that these scans were taken 3 times per patient at different periods in their treatment: one in the beginning, one roughly in the middle, and one near the end. This means that there is some temporal information present in the datasets, but this has been encoded in the explanatory variables. For instance: there is a variable in the MRI dataset that shows the size change of the tumour between the first and second scan. In this way we can treat this as a regular explanatory variable and we don't have to worry about the temporal aspect. \\ \\
The dependent variable is a binary outcome based on whether the patient got a pathologically complete response. For a more in-depth explanation on what this means see appendix \ref{app:cancer-staging}. For now, we can view a positive outcome as a patient who has recovered from the cancer, and a negative outcome as a patient that has not. The aim is now to build a model that receives the explanatory variables from the scans as input, and predicts the outcome for that patient.
\subsection{The results}
The resulting AUC's for the individual and integrated datasets are shown respectively in tables \ref{tab:evaluation-auc-individual} and \ref{tab:evaluation-auc-integrated}. We can clearly see that the model for the individual MRI dataset contains the most predictive information out of the three datasets. This however doesn't mean that the other datasets are useless. Indeed, when we integrate the other dataset with the MRI dataset we get even better performances. From table \ref{tab:evaluation-auc-integrated} we can see that in this case the intermediate integration method outperforms the other strategies regardless of which datasets are used. \\ \\
I have also included an overview of the model parameters that were selected (on average) by each strategy. The values in this table show the weights given to each corresponding variable in the model. A positive weight indicates that a higher value for this variable increases the chance of a positive outcome. A negative weight correlates the variable with a decreased probability of positive outcome. The overview is shown in table \ref{tab:evaluation-model-parameters}. The first column shows the name of the selected variable, these are imaging features. The next three columns show the model parameters for the individual models. The last two columns show the parameters that were selected by the integrated models. It is interesting to see how all models tend to select the same kind of variables and give them similar weights. This shows us that these variables indeed do contain some predictive information. Notice that the intermediate model selects fewer variables than the other models, while still outperforming them. It removes those variables that have very small weights in other models and thus acts as a very strict variable selector. This is no surprise if we remember how the intermediate integration method uses two steps of variable selection to perform the integration (chapter \ref{cha:integration}, section \ref{sec:integration-intermediate}).

\begin{table}
	\centering
	\begin{tabular}{lc}
		\toprule
		Dataset & AUC \\
		\midrule
		DWI & 0.67 \\
		MRI & 0.75 \\
		SUV & 0.65 \\
		\bottomrule
	\end{tabular}
	\caption{AUC for models of individual datasets}
	\label{tab:evaluation-auc-individual}
\end{table}
\begin{table}
	\centering
	\begin{tabular}{lcccc}
		\toprule
	 & All & DWI+MRI & DWI+SUV & MRI+SUV \\
		\midrule
		Early integration & 0.76 & 0.82 & 0.69 & 0.74 \\
		Intermediate integration & 0.79 & 0.83 & 0.78 & 0.75 \\
		Late integration & 0.73 & 0.78 & 0.66 & 0.70 \\
		\bottomrule
	\end{tabular}
	\caption{AUC for models of various combinations of integrated datasets}
	\label{tab:evaluation-auc-integrated}
\end{table}


\begin{table}
	\rowcolors{1}{white}{lightgray}
	\centering
	\begin{tabular}{lccccccc} 
		\toprule
		\multicolumn{1}{c}{}& \multicolumn{3}{c}{Individual} & \multicolumn{2}{c}{}&  \multicolumn{2}{c}{Integrated}\\ 
		\cmidrule{2-8}
		Variable & DWI    & MRI & SUV & & & Early    & Intermediate\\ 
		\midrule
		ADCratioavg\_TP1TP3 		& 0.69	& 		&  		& & & 1.09		& 1.19	\\ 
		ADCratiohigh\_TP1TP3 		& 0.08	&      	&  		& & & 0.50		& 0.62	\\ [10pt]
		DeltaSphere\_TP2TP3perc 	&      	& 1.67  &  		& & & 1.35		& 1.44  \\
		DeltaSphere\_TP1TP3perc 	&      	& 0.48  &  		& & & 1.78		& 1.64  \\
		Volume\_TP2 				&  		& -0.99 &   	& & & 			& 		\\
		Volume\_TP3 				&  		& -0.24 &   	& & & -0.173	& -0.94	\\ [10pt]
		RIdiameter\_TP2TP3 			&  		&      	& 0.16  & & & 			& 		\\
		Diameter\_TP3 				&  		&      	& -0.88 & & & -0.01		& 		\\
		SUVmax\_TP2 				&  		&      	& -0.88 & & & -3.30		& -4.41	\\
		deltadiameter\_TP1TP2 		&  		&      	& -0.32 & & & 			& 		\\
		RISUVpeak\_TP1TP2 			&  		&      	&  		& & & 0.32		& 		\\
		RITLG\_TP2TP3 				&  		&      	&  		& & & -0.08		& 		\\
		\bottomrule
	\end{tabular}
	\caption{Overview of the model parameters (weights)}
	\label{tab:evaluation-model-parameters}
\end{table}

\section{Evaluating a cox proportional hazards model}
\label{sec:evaluation-coxph}
The evaluation of a cox proportional hazards model is different compared to logistic regression\cite{newson2010comparing}\cite{harrell1996tutorial}. This is because, as we have seen in chapter \ref{cha:cox}, we normally don't directly compute survival times from a cox proportional hazards model. This means we cannot compare them to the real survival times. However we can still evaluate the models based on statistics that indicate significance.

\subsection{Predictions for survival models}
Recall the hazard ratio from section \ref{sec:cox-proportional-hazards-model}. If we have a model computed ($\beta's$ defined) and we have a test sample (with values $X_{i}$ for the explanatory variables), we could compute the following prediction quantity:
\begin{equation}
\begin{split}
prediction = exp(\sum_{i=0}^{N}X_{i}\beta_{i})
\end{split}
\end{equation}
where
\begin{itemize}
	\item $exp(x)$ is the exponential of x
	\item $X_{i}$ are the values for the explanatory variables
	\item $\beta_{i}$ are the model coefficients
\end{itemize}
 We can view this as the hazard ratio between the test sample and an imaginary patient that has the value $0$ for all explanatory variables which we will call the baseline patient (subscript $b$):
 \begin{equation}
 \begin{split}
 \frac{\lambda_{i}(t)}{\lambda_{b}(t)} 
 = \frac{\lambda_{0}(t)e^{X_{i1}\beta_{1} + ... + X_{iN}\beta_{N}}}{\lambda_{0}(t)e^{0\beta_{1} + ... + 0\beta_{N}}}
 = e^{(X_{i1}-0)\beta_{1} + ... + (X_{iN}-0)\beta_{N}} = exp(\sum_{j=0}^{N}X_{ij}\beta_{j})
 \end{split}
 \end{equation}
 What this quantity tells us is whether the risk of our test patient is higher or lower than the risk of the baseline patient. This does not tell us anything in the absolute sense about survival times, but we can compare this prediction value in a relative way between patients. \\ \\
 We could, as we did in section \ref{sec:evaluation-logisticregression}, compute this prediction value for all patients in our dataset using cross-validation. We now want to test the predictive value of these predictions. To do this we construct a new survival model, but this time there is only one explanatory variable and that is our vector of predictions. If we can show that this variable is significant in predicting the survival, then our original model is also significant. The significance test we will use is called the Wald Test.
 
 \subsection{The Wald test}
The Wald test\cite{wikiwald} is a statistical test that can be used to test the significance of an estimated parameter. The Wald test can be used on a single parameter or on a vector of parameters at once. In the case of one parameter $\beta$ the formula for the Wald test is:
\begin{equation}
\begin{split}
\frac{(\hat{\beta}-\beta_{0})^{2}}{var(\hat{\beta})} \sim \tilde{\chi}^{2}_{1}
\end{split}
\end{equation}
where
\begin{itemize}
	\item $\hat{\beta}$ is the maximum likelihood estimate we calculated and want to test
	\item $\beta_{0}$ is the value of $\beta$ if the null-hypothesis is true
	\item $var(\beta)$ is the variance of $\beta$
	\item $\tilde{\chi}^{2}_{1}$ is the chi-squared distribution with one degree of freedom
\end{itemize}
When we are testing the significance of a coefficient in our survival model we have to compare it against a so called null-hypothesis. In this case the null-hypothesis is the model where all coefficients are equal to zero. The values for $\hat{\beta}$ and $var(\beta)$ are defined by the model. Intuitively the Wald test computes the distance between the obtained value for the parameter and the parameter value under the null-hypothesis, relative to the variance of the parameter. If the variance is large, then we are unsure about the value for the parameter that we calculated and it is harder to reject the null-hypothesis. An example is demonstrated on figure \ref{fig:evaluation-wald-test}. \\ \\
\begin{figure}
	\centering
	\includegraphics[scale=.7]{images/wald_test}
	\caption{Example Wald test variance visualisation. Both curves give the same maximum likelihood estimate $\hat{\beta}$. The red curve however has low variance, providing good evidence to reject the null-hypothesis with value $\beta_{0}$. The blue curve has high variance, in this case $\hat{\beta}$ does not significantly differ from $\beta_{0}$ and we cannot reject the null-hypothesis.}
	\label{fig:evaluation-wald-test}
\end{figure}
The Wald test can tell us something about the significance of the predictions vector and therefore about the predictive ability of our initial model. We can go one step further and compute a p-value\cite{pvalue1}\cite{wikip} for the wald-test statistic. This p-value contains the exact same information as the wald-test statistic, but it is a very well known quantity statistics so we will add it for completeness.
\section{Case study 2: predicting survival outcomes}
\label{sec:evaluation-predictingsurvival}

\section{Conclusion}
\label{sec:evaluation-conclusion}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
