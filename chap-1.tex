\chapter{Machine Learning Models}
\label{cha:1}
In this chapter the different machine learning methods that are used are being explained in detail. This is necessary because later on I will explain different integration strategies, and these often depend on the specific structure of a machine learning method. The first section explains what Generalized Linear Models are. This name actually indicates a whole collection of different methods, but there is one method that will be of particular interest to us, called 'logistic regression'. This section will be the most extensive as it explains a lot of concepts that can be reused later on, and also because this method is the one that I have used the most in my thesis. \\
The next section will explain the Cox Proportional Hazards method. This method is used when the data we are dealing with is so-called survival data. \\
Lastly I will briefly mention Neural Networks and why they are important.

\section{Generalized Linear Models}
When we think of classical linear models, we can imagine a set of numeric explanatory (or input) variables and a numerical dependent (or output) variable. By making a linear combination of the explanatory variables we attempt to estimate a value for the dependent variable. Depending on the type of dependent variable the linear method gets a different name. In the following sections I will outline several of them.

\subsection{Linear Regression}
The simplest version of a linear model is called linear regression. In this case the input variables are combined using a linear combination, and the result of this calculation is immediately used as the final estimate. \\
//TODO MATH \\
For the other linear methods we will define a function each time that is applied to the result of the linear combination. We could do the same for linear regression and say that the applied function is the identity. We could schematize this compuation as follows: \\
//TODO SCHEMA \\
\subsection{Linear Classification}
The next method is called linear classification. The difference with linear regression is that we have a different type of output variable. In a classification task we want to predict a class from a list of potential classes. For instance, we could try to predict whether tomorrow will be a sunny day or not. Notice that there are only 2 possible outcomes: 'sunny' or 'not sunny' and we could represent these outcomes as 0 and 1 in our model. This form would be called binary classification because we have 2 possible classes. It is very easy to extend this method to multi-class classification.\\
The computation in this method starts out exactly the same, combining the input variables using a linear combination. Next, we have to define a threshold to indicate which examples belong to one class or another. In the case of binary classification we would define 1 threshold, and if the result of the linear combination is higher than the threshold we would predict one class. If it is lower, we would predict the other class. We could represent this computation with the following formula and schema: \\
//TODO MATH AND SCHEMA\\

\subsection{Logistic Regression}
The third method I want to present is called logistic regression. In this case, the output variable we want to predict comes from a binomial distribution. This means that they are the result of a probabilistic event. An example would be tossing a coin and checking whether the result is heads or tails. While the outcome is binary (heads or tails) we know that there is an underlying probability for the coin to be heads or tails, and we would like to know this probability. \\
The idea is still the same. We will make a linear combination of the input variables. However this time we will use a logistic function to produce our estimate. The logistic function is a function that maps real numbers onto the range $[0,1]$. This result can then be interpreted as an estimate for the probability. \\
The logistic regression method is the one that will be most widely used throughout this thesis.

\subsection{Another item}
\lipsum[56-57]

\section{Conclusion}
The final section of the chapter gives an overview of the important results
of this chapter. This implies that the introductory chapter and the
concluding chapter don't need a conclusion.

\lipsum[66]

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
