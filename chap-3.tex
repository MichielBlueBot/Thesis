\chapter{Integration Strategies}
\label{cha:integration}

\section{Introduction}
\label{sec:integration-introduction}
Now that we have established a firm understanding of the methods we will use to create predictive models, it is time to turn our attention to the integration step. In this chapter we will present three different integration strategies. We will start by providing a description of the datasets that we will use to build our models (\ref{sec:integration-description}), followed by the integration strategies that show how we can combine these datasets to create predictive models. The first strategy is called early integration (\ref{sec:integration-early}) and is based on concatenation of datasets. The second strategy is called late integration (\ref{sec:integration-late}) which is based on ensemble learning. The last strategy is intermediate integration (\ref{sec:integration-intermediate}) which makes extensive use of the variable selection present in the lasso regularization (see section \ref{sec:glm-regularization}).

\section{Description of the data}
\label{sec:integration-description}
In order to integrate the datasets, they have to adhere to a number of requirements. Suppose we have a set of D datasets, each dataset can be represented by a matrix where the rows are the samples and the columns are the explanatory variables. Each dataset can have different explanatory variables, but they all need to have rows that can be linked across datasets, for instance using a unique patient identifier. Let me give a concrete example: imagine we have a set of 200 cancer patients. Each of these patients has had images taken of the cancer by different machines: an MRI scan, a PET scan, ... and they also had a bloodtest done measuring different prote\"{i}ne levels in the blood (an ELISA test). Each of these methods will yield a dataset: one for each imaging scan and one for the blood test. The explanatory variables are obviously different for each dataset: the imaging datasets will have image features like blobs and pixel intensities, while the bloodtest will have values that represent the prote\"{i}ne levels in the blood. Each dataset will however have a single row per patient, uniquely identified by that patients ID. This is not true in the case of missing data (e.g. a patient missed his MRI scan appointment). But there are ways of dealing with this: we can try to estimate the missing values using clustering techniques, or we can simply leave the patients with missing data out of the study. \\ \\
Now that we have our input data, we need a target function (dependent variable). In the example case this could be whether the patient fully recovers from the treatment or not. This variable is supposed to have a binomial distribution and thus we could use logistic regression to try and estimate this variable. The task of integration is now the following: how do we combine the samples across the different datasets to come up with a predictive model that uses the information in all the datasets? In the following sections we will present three ways to do this: the early integration method which is concatenation, the late integration method that is based on an ensemble of models, and the intermediate integration method that takes advantage of variable selection present in the lasso regularization.

\section{Early integration}
\label{sec:integration-early}
The first integration method is called early integration. In this method we are simply going to concatenate the data for each sample. Figure \ref{fig:integration-early} shows the schema for early integration of data sources. A new large dataset is constructed by concatenating the individual datasets by matching sample. The number of explanatory variables is now equal to the sum of explanatory variables of all individual datasets. This integrated dataset can then be used to train a predictive model using techniques seen in previous chapters (\ref{cha:glm},\ref{cha:cox}).
\begin{figure}
	\centering
	\includegraphics[scale=.8]{images/early_integration}
	\caption{Scheme for early integration}
	\label{fig:integration-early}
\end{figure}
\section{Late integration}
\label{sec:integration-late}
In late integration, as the name suggests, we will combine the different datasets at the end of the learning process. The schema for late integration is shown on figure \ref{fig:integration-late}. First, a model is learned for each dataset individually. This gives rise to $D$ different models. To reuse the example from section \ref{sec:integration-description}, there will be a model for MRI data, PET data, etc. When we want to predict an outcome for a new sample (patient) we will present each model with the corresponding input from the patient. Each model will then compute an output and these are combined using a linear combination. If we have no preference for any model we could simply compute the average output. This average would then be our final output for the integrated model. In this case we cannot represent the final model by some model parameters (weights) but rather we have to view the full set of $D$ models as well as the linear combination we chose as the full integrated model. \\ \\
There is an analogy between late integration and the ensemble averaging technique in machine learning. Ensemble learning \cite{dietterich2002ensemble}\cite{dietterich2000ensemble}\cite{wikiensemble} means that instead of just building one model for a dataset, we build several models for the same dataset and then average their outcomes. Late integration takes this idea but applies it to a set of data sources instead of just one. \\ \\
The thought behind this model is that we try to learn as much as possible from each dataset individually, before we combine them. An example of this is that the variable selection by the lasso regularization (section \ref{insec:glm-lasso}) now has the opportunity to select the best variables for each dataset individually. While in the case of early integration, all the variables from the datasets are competing against each other at the same time to make it into the final model.
\begin{figure}
	\centering
	\includegraphics[scale=.8]{images/late_integration}
	\caption{Scheme for late integration}
	\label{fig:integration-late}
\end{figure}
\section{Intermediate integration}
\label{sec:integration-intermediate}
Intermediate integration is a novel integration technique that tries to take advantage of the variable selection when using a lasso penalty. The schema for intermediate integration is shown on figure \ref{fig:integration-intermediate}. It starts out the same way as late integration, for each dataset we compute an individual model. However, instead of using the output of these models we will simply look at the variables that were chosen by each model to be informative. We will then go back to the original datasets and extract only those explanatory variables and concatenate them together into a new integrated dataset. Lastly, we use this integrated dataset to learn a final model. \\ \\
The intermediate integration can be seen as a two-step learning process. First we preprocess the datasets by computing individual models and we extract only the informative variables. Then we will train our final model on the reduced dataset. This is very advantageous if the initial datasets have a huge amount of explanatory variables, as the preprocessing step will reduce this amount drastically while still keeping as much useful information as possible.
\begin{figure}
	\centering
	\includegraphics[scale=.8]{images/intermediate_integration}
	\caption{Scheme for intermediate integration}
	\label{fig:integration-intermediate}
\end{figure}
\section{Conclusion}
\label{sec:integration-conclusion}
In this chapter we have shown the issue of large and high-dimensional datasets that we are currently facing. We have presented three different strategies to deal with this issue. The early integration method is the na\"{i}ve method that simply concatenates the datasets together. The late integration method uses an analogous technique to ensemble learning in order to extract as much information as possible. Lastly, the intermediate integration technique introduces a two-step learning method that takes advantage of variable selection to reduce the dimensionality of the datasets.
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
